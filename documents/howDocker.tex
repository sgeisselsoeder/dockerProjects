\documentclass[a4paper, twoside, 11pt]{article}
% deutsche Silbentrennung
% \usepackage[ngerman]{babel}
% wegen deutschen Umlauten
\usepackage[utf8]{inputenc}

\usepackage{a4}

% fuer links im dokument
\usepackage[colorlinks, linkcolor = black, citecolor = black, filecolor = blue, urlcolor = blue]{hyperref}

% for code examples
\usepackage{listings}
% for equations with multiple lines
\usepackage{amsmath}

% for including graphics
\usepackage{graphicx}
% for floatbarriers
\usepackage{placeins}

\begin{document}

\title{Using Docker for your analysis in KM3NeT}
\author{Stefan Geißelsöder\\
ECAP, FAU Erlangen-Nürnberg}
\date{\today}
\maketitle

\section{Why should you use Docker for your analysis?}

At some point there is a demand to repeat, change or externally check an analysis. 
Using Docker will simplify your work in these cases. 

You can be sure that you will be able to reproduce the parts 
of your analysis that are covered by Docker even if the 
computer/cluster/cloud/whatever you used to obtain your result in the first place 
has been updated or when the specific machine is not available anymore.
You can rely on getting the same results, 
regardless of changes in software versions. 
Reproducibility is a major benefit of using Docker. 

Another advantage is that it also helps a student or colleague that is to reuse your work. 
He or she will be able to start where you left of and develop further 
without first having to worry about the details of how to compile, install or run your work.  


\section{Can you use Docker for your analysis?}

For almost all analyses: yes. \\
The ideal use case is when your analysis can run on a single computer. 
Foreseeable complications can be dealt with:
\begin{itemize}
 \item If you use many compute nodes e.g. to prefilter data in a customized way, 
 you can run the evaluation on the prefiltered data with Docker. 
 Alternatively, if the cluster/cloud you use for filtering also supports Docker, 
 you can use a second Docker image for the filtering process. 
 \item If you require dedicated hardware like GPUs, some additional work is required, 
 but the general procedure stays the same. 
 See e.g. \cite{nvidia:docker} for more details on this. 
\end{itemize}

\section{How can you use Docker?}

We discuss a toy example in C++ on Linux here, 
but the steps translate to different languages and operating systems. 
This tutorial can be obtained at \cite{gith:tutorial}. 
As a side remark, even though the example in Section \ref{sec:toyexample} 
may be sufficient to run your own analysis with docker, 
please also have a look at Section \ref{sec:installSoftware}  
for further relevant aspects.

\subsection{Prerequisites}
\label{sec:isDockerRunning}

This tutorial assumes that you have your input files and your code available and that you have Docker 
installed\footnote{Information on how to install Docker can be found e.g. at 
\cite{digoc:install}
or \cite{dock:install}
.} and working. 

You can check if Docker is working correctly by typing ``docker ps'' into your favorite shell. 
If the output starts similar to ``CONTAINER ID \hspace{0.5cm} IMAGE \hspace{0.5cm} COMMAND``, you are good to go. 
If it fails, but ''sudo docker ps`` works, your user is not part of the Docker group. 
You can either prepend sudo to all docker commands or add your user to the Docker group (see \cite{digoc:install}). 

\subsection{A toy example}
\label{sec:toyexample}
Everything required for our example is located in the folder toyExample. 
Within toyExample there is a folder named externalInput which contains the prefiltered input files, 
a folder named myAnalysis which contains the required source code, Makefiles, scripts, etc., 
a folder externalOutput where the output of the analysis will be stored, 
a script called runMyAnalysis.sh that will execute the analysis and a file called Dockerfile. 

\subsubsection{The Dockerfile}
\label{sec:dockerfile}

The Dockerfile encapsulates the own analysis software together 
with all software dependencies, aware or not, 
including even the basic operating system.
It is used to create the new Docker image, that executes the analysis. 
In the example the Dockerfile reads: 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
FROM km3net/km3base:20160909
MAINTAINER S. Geisselsoeder <stefan.geisselsoeder@fau.de>

COPY myAnalysis /myAnalysis
COPY externalInput /input
COPY runMyAnalysis.sh /runMyAnalysis.sh

RUN make -C /myAnalysis
RUN chmod 755 runMyAnalysis.sh

ENTRYPOINT ["/runMyAnalysis.sh"]
\end{lstlisting}
The Dockerfile starts with the keyword ''FROM``, 
stating from which base image this image will be derived.
Typically these base images provide the operating system and some adaptations to it. 
In our case it's based on CentOS, 
specifically extended to provide software typically required for analyses in KM3NeT. 
''km3net`` denotes the name of the collaboration on Dockerhub\cite{dockerhub:km3net}
''km3base`` is the name of the base image used for analyses in KM3NeT and 
''20160909`` is the tag identifying which version of the base image is to be used. 
The common tag ''latest`` is not used intentionally, as this could change the basis of the analysis unnoticed. 
MAINTAINER is again a keyword, used to identify and contact the author. 

\label{sec:addingYourCode}
To have the scripts, source code and input files for the analysis available in the Docker image, the keyword ''COPY`` is used.
The first statement 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
COPY myAnalysis /myAnalysis
\end{lstlisting}
copies the local folder myAnalysis, which is located within the toyExample folder. 
It contains the source code of the toy example.
The destination /myAnalysis specifies where the folder will be available when the Docker container is run. 
The same is done for the input files by
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
COPY externalInput /input 
\end{lstlisting}
and for the script ''runMyAnalysis.sh`` that will be used to execute the example. 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
COPY runMyAnalysis.sh /runMyAnalysis.sh
\end{lstlisting}
This script is explained in Section \ref{sec:runmyanalysis}.

''RUN`` is a keyword in the Dockerfile that tells Docker to execute this statement once when creating the image. 
Hence the command 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN make -C /myAnalysis
\end{lstlisting}
compiles the source code within the Docker image, producing the binary file. 
This is analogous to executing ''make`` within the folder toyExample/myAnalysis/, 
but in contrast to the command executed on the host computer, 
it uses the software versions provided by the base image 
(and potential changes made by previous commands). 
As this is a well defined state that cannot change over time, 
the same result will be obtained when the command is executed in the future. 
The command
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN chmod 755 runMyAnalysis.sh
\end{lstlisting}
changes the access rights for the file, just as it would do on a host machine. 
To automatically run the analysis by default when the image is executed, the command
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
ENTRYPOINT ["/runMyAnalysis.sh"]
\end{lstlisting}
is used.

\subsubsection{runMyAnalysis.sh}
\label{sec:runmyanalysis}
For simplicity and clarity we incorporate the workflow of the analysis 
into the docker image using a bash script. 
This is what ''runMyAnalysis.sh`` is for. In this example it reads:
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
#!/bin/bash
echo "Using inputs from /input"
ln -s /input /myAnalysis/input

echo "Executing analysis"
cd /myAnalysis
./bin/myAnalysisBinary

echo "Collecting all results to /output/"
mv outputfile.txt /output/
chmod -R 777 /output/
\end{lstlisting}
This file links the input to the path where the binary expects it to be 
(we could also copy it), 
enters the path where our software is located in 
the file system within the Docker image, 
executes the already compiled software 
and moves the resulting output to a desired location.
The compilation happened in the Dockerfile at ''RUN make -C /myAnalysis``, 
but it could as well be done here.
The difference is that the statement in the Dockerfile 
is executed once when creating the image, while a ''make`` in this script will be executed 
every time the image is started. 
Hence we prefer to have it in the Dockerfile. 
The change of access rights in the last line is one of the simplest solutions 
to deal with different users within Docker and on the host system. 

Obviously this is a simple setup, but every analysis should be able to be executed by some sort of scripts. 

\subsubsection{Creating the Docker image}
\label{sec:createImage}
The information in Sections \ref{sec:dockerfile} and \ref{sec:runmyanalysis} 
is already sufficient to create a new Docker image. 
It can be done by executing
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker build -t my_analysis:v1 .
\end{lstlisting}
in the folder toyExample. 
''my\_analysis`` is the name we want the Docker image to have, 
''.`` is the relative path to the Dockerfile.

Docker will now automatically download all parts of the km3net base image 
(we specified that in the Dockerfile)
which are not present already and then build the new image. 
Subsequent calls of this command will be faster, as no additional parts have to be downloaded. 

If everything succeeded, the command ''docker images`` 
should now list at least the base image km3net/km3base 
with the version tag specified in the Dockerfile and 
the new image my\_analysis with tag v1. 
A unique version tag is essential, as multiple iterations of an analysis could be produced in the future. 
To automatically ensure such a tag, the script ''createDocker.sh`` can be used.
It executes the docker build command and adds the date as version tag. 


\subsubsection{Running the Docker container}
\label{sec:runContainer}

After Section \ref{sec:createImage} a Docker image with the example analysis should be available. 
Actually running the example in a Docker container\footnote{The object running the image 
(and taking care of various other things in the background) is called a container. 
One could think of it as being slightly similar to a binary file (image) and the process running it (container).} 
is done using the command 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker run -v `pwd`/externalOutput:/output my_analysis:v1
\end{lstlisting}
While ''docker run my\_analysis:v1``
would be enough to run the example, the output is stored in folder 
''/output/`` as specified in Section \ref{sec:runmyanalysis}.
Without any further options this folder lives within the Docker container, but is not accessible from the outside. 
Therefore the option -v can be used to mount a folder from the host system to a folder in the container. 
With ''-v `pwd`/externalOutput:/output``, all files that are written within the container to /output 
are actually stored to the folder ./externalOutput on the host system. 
We write `pwd`/externalOutput instead of ./externalOutput because the 
paths may not be relative but have to be absolute when using the option -v. 

When we run the Docker container, we see the output that would be printed when 
executing the script runMyAnalysis.sh directly, and afterwards the output is found in folder ./externalOutput. 
It should be noted that no changes made to the container by the execution of our analysis 
are stored to any image. 
Therefore all desired output has to end up in the designated output folder (or another mounted volume). 
Even though it is not mandatory for Docker, 
we propose to use the convention of /input and /output folders for Docker images. 
This convention simplifies their usage significantly. 

We have now successfully ported our analysis to Docker, ensuring that it can easily be run  
on a different machine and will reliably yield the same results. 



\subsection{Installing and updating software}
\label{sec:installSoftware}
As the base image is derived from CentOS, one could update the contained software using yum. 
But \textbf{WE WANT TO AVOID CHANGING THE SOFTWARE OF THE BASE IMAGE!}

To achieve reproducibility, one should ideally rely on the exact software installed in the base image and only add your own code. 
If you require additional or updated third-party software, the best (and for you most convenient) way 
is to kindly ask the maintainer of the base image for an updated version. 
This doesn't cause unnecessarily effort, 
as others will likely require this change at some point in time, too. 

\subsubsection{Installing software yourself}
Anyway, if you have a reason not to do this (the maintainer might be horribly busy, on vacation for six weeks, ...) 
you can also install additional third party software yourself, 
but \textbf{\textit{only}} if the version number of the software is specified. 
This means for example that you may not use 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN yum -y install boost 
\end{lstlisting}
but instead must use 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN yum -y install boost-1.53.0
\end{lstlisting}
The flag ''-y`` is required as creating the Docker image isn't an interactive process. 
Hence the command would fail without it. 
If you don't know what version numbers are available for a certain software, you can use 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker run -it --entrypoint /bin/bash km3net/km3base:20160909
\end{lstlisting}
(Replace the tag with your base image) \\
This opens an interactive shell in a container running the base image. 
Then type ''yum list boost`` to see the available versions. 

Since even specifying the version of software is not always an option, 
the next best solution would be to download a fixed state of third-party software 
and to add it to the image similar to what you would do with your own code, 
see Section \ref{sec:addingYourCode}. 
In this case you have to preserve the downloaded fixed state together with the Dockerfile and your own code, 
as it has just become part of your analysis software. 


\subsubsection{Smaller images after installing software}
If you need to install software yourself, here are a few hints to produce smaller images: \\
- Install all required software in one command instead of multiple. Hence 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN yum -y install boost-1.53.0 git-1.8.3.1
\end{lstlisting}
is to be preferred over
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN yum -y install boost-1.53.0
RUN yum -y install git-1.8.3.1
\end{lstlisting}
The reason for this is that each of these commands creates an individual layer. 
Since the status of the image is stored for each layer, a single command requires less space than multiple commands. \\
- It is also a good idea to add ''\&\& yum -y clean all`` at the end of an installation, 
as unnecessary intermediate and cached files will be removed. 
The final command should therefore be
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
RUN yum -y install boost-1.53.0 git-1.8.3.1 && yum -y clean all
\end{lstlisting}


\subsection{Dealing with large input files}
Instead of copying the input files to the image as we did in Section \ref{sec:dockerfile}, 
we could have also mounted a volume (using -v) as we did for the output files. 
The advantage of copying the input to the image is that it is stored together with the image, 
making it trivial to reproduce results in the end. 
For very large input files this becomes a disadvantage, as these would have to be stored, too. 
The best possible approach in such a situation would be to try to extract 
the information relevant for this analysis, e.g. by removing unused raw data from the files. 
Alternatively, if the data ain't a custom selection 
but a standard set used by many analyses, one might think about including it into the base image. 
The last option should be not to include the input into the image at all, but to mount it. 
Even though the exact definition of ''very large`` (and especially ''too large``) 
depends on the actual situation and will certainly change over time, 
as a very coarse guide for orientation, input of up to 2 GB should be included in an image. 
Nevertheless, it is always a good idea to remove unnecessary parts from the input 
as long as the original data can be uniquely identified. 


\subsection{Removing deprecated containers and images}
When beginning to experiment with Docker, it is likely that a multitude of containers and images will  
soon occupy a non-negligible amount of space on the machine. 
As we won't need our first baby-steps anymore, we can get rid of these images. 
Since Docker doesn't allow the removal of images that are still used in any container, 
we first have to get rid of deprecated containers. 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker ps -a
\end{lstlisting}
shows a list of all containers we have run. 
The relevant identifier for a container is its ID, e.g. ''d6cc1ff47c72``
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker rm d6cc1ff47c72
\end{lstlisting}
removes this container. 
The file ''cleanContainer.sh`` in the main folder removes all found containers. 
Don't worry, removing containers does not hurt the images. 
Once all containers referring to an image are removed, the image itself can be removed.
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker images
\end{lstlisting}
shows the list of stored images. 
Note that the displayed size of the images also includes base images. 
Hence your own image is displayed \textit{at least} as big as the base image, 
but, due to the layered structure of Docker images, in reality only requires approximately the difference of the two numbers. 
To remove an image one again uses its identifier, e.g. ''49462b1e4dcb``. 
The command would be 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker rmi 49462b1e4dcb
\end{lstlisting}

\section{Using images with Dockerhub}

Once an image has been created it can be uploaded to the 
KM3NeT Dockerhub account\footnote{This has to be done by the current administrator of the KM3NeT account.}. 
Beware however to only include input data that should become public in uploaded images. 
For not (yet) public data, either mount the input instead of including 
it to the image (see Section \ref{sec:addingYourCode}) 
or do not upload the image (yet), but store it on protected storage. 

The benefit is that sharing and executing the analysis becomes trivial. 
For instance to run the toy example it is sufficient to execute 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker run -v `pwd`/extOutput:/output km3net/my_analysis:20170306
\end{lstlisting}
This does not require you to build the image on your machine. 
The corresponding image has been created exactly as described in Section \ref{sec:toyexample} 
and is downloaded like the base image has been in Section \ref{sec:createImage}. 

As we understand now how that is achieved, 
it is simple to use the same scheme for another analysis. 
We can execute the multiscale analysis 
\cite{geisselsoeder:phd, gith:multiscale} with the command 
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
docker run -v `pwd`/extOutput:/output km3net/multiscale:20170307
\end{lstlisting}
The project creating this image can also be found at \cite{gith:tutorial}.

\section{Summary}

Using Docker for your analysis is a moderate one-time effort, 
but it benefits yourself (easy to fulfill requests), 
your colleagues (easier to get involved in your project) 
and the collaboration (reliable reproducibility).

\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,frame=single]
git clone https://github.com/sgeisselsoeder/dockerProjects.git temp
cd temp/toyExample
docker build -t my_analysis:v1 .
docker run -v `pwd`/externalOutput:/output my_analysis:v1
\end{lstlisting}


\bibliography{dockerRefs}
\bibliographystyle{alpha}

\end{document}

